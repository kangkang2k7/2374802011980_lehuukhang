# 2374802011980_lehuukhang
#lab2
BÃ i toÃ¡n N-Queens yÃªu cáº§u Ä‘áº·t N quÃ¢n háº­u trÃªn bÃ n cá» kÃ­ch thÆ°á»›c N x N sao cho:
KhÃ´ng cÃ³ hai quÃ¢n háº­u nÃ o náº±m trÃªn cÃ¹ng hÃ ng.
KhÃ´ng cÃ³ hai quÃ¢n háº­u nÃ o náº±m trÃªn cÃ¹ng cá»™t.
KhÃ´ng cÃ³ hai quÃ¢n háº­u nÃ o náº±m trÃªn cÃ¹ng Ä‘Æ°á»ng chÃ©o chÃ­nh hoáº·c phá»¥.
ğŸ§° CÃ´ng cá»¥ sá»­ dá»¥ng
Python 3.x
ThÆ° viá»‡n: numpy, random
CÃ i Ä‘áº·t thÆ° viá»‡n (náº¿u chÆ°a cÃ³)
pip install numpy
ğŸ§  Giáº£i thuáº­t chÃ­nh
1. is_valid_state(state, num_queens)
Kiá»ƒm tra xem má»™t tráº¡ng thÃ¡i Ä‘Ã£ hoÃ n chá»‰nh chÆ°a (Ä‘Ã£ Ä‘áº·t Ä‘á»§ N quÃ¢n háº­u).

2. get_candidates(state, num_queens)
Tráº£ vá» danh sÃ¡ch cÃ¡c cá»™t há»£p lá»‡ mÃ  quÃ¢n háº­u cÃ³ thá»ƒ Ä‘áº·t tiáº¿p theo, dá»±a trÃªn:
Cá»™t Ä‘Ã£ bá»‹ chiáº¿m.
Hai Ä‘Æ°á»ng chÃ©o Ä‘Ã£ bá»‹ chiáº¿m.
3. search(state, solutions, num_queens)
Thuáº­t toÃ¡n Ä‘á»‡ quy backtracking:
Gá»i Ä‘á»‡ quy vá»›i tá»«ng á»©ng viÃªn há»£p lá»‡.
Náº¿u Ä‘áº¡t tráº¡ng thÃ¡i Ä‘áº§y Ä‘á»§ â†’ thÃªm vÃ o danh sÃ¡ch lá»i giáº£i.
Náº¿u khÃ´ng há»£p lá»‡ â†’ quay lui.
4. solve(num_queens)
Gá»i hÃ m search vÃ  tráº£ vá» táº¥t cáº£ cÃ¡c lá»i giáº£i tÃ¬m Ä‘Æ°á»£c.
â–¶ï¸ CÃ¡ch cháº¡y chÆ°Æ¡ng trÃ¬nh
File n_queens.py:
python n_queens.py
Khi Ä‘Æ°á»£c há»i:
Nhap vao so quan hau N = 
â†’ Báº¡n cÃ³ thá»ƒ nháº­p 4 hoáº·c 8, vÃ­ dá»¥:
Nhap vao so quan hau N = 8
ğŸ§ª VÃ­ dá»¥ Ä‘áº§u ra
Nháº­p: N = 4
 Tá»•ng sá»‘ lá»i giáº£i tÃ¬m Ä‘Æ°á»£c: 2
Lá»i giáº£i 1:
QuÃ¢n háº­u táº¡i dÃ²ng 0, cá»™t 1
QuÃ¢n háº­u táº¡i dÃ²ng 1, cá»™t 3
QuÃ¢n háº­u táº¡i dÃ²ng 2, cá»™t 0
QuÃ¢n háº­u táº¡i dÃ²ng 3, cá»™t 2

Lá»i giáº£i 2:
QuÃ¢n háº­u táº¡i dÃ²ng 0, cá»™t 2
QuÃ¢n háº­u táº¡i dÃ²ng 1, cá»™t 0
QuÃ¢n háº­u táº¡i dÃ²ng 2, cá»™t 3
QuÃ¢n háº­u táº¡i dÃ²ng 3, cá»™t 1


#LAB CNN

## CÃ¢u 1: Thay Ä‘á»•i sá»‘ lÆ°á»£ng epoch

- **YÃªu cáº§u:** TÄƒng sá»‘ epoch tá»« 5 lÃªn 10.
- **Káº¿t quáº£:** Äá»™ chÃ­nh xÃ¡c tÄƒng, loss giáº£m dáº§n qua cÃ¡c epoch.
- **Giáº£i thÃ­ch:** TÄƒng epoch giÃºp mÃ´ hÃ¬nh há»c ká»¹ hÆ¡n, giáº£m underfitting, nhÆ°ng quÃ¡ nhiá»u cÃ³ thá»ƒ gÃ¢y overfitting.

```python
for epoch in range(10):  # tÄƒng tá»« 5 lÃªn 10
    # Huáº¥n luyá»‡n vÃ  tÃ­nh loss, accuracy
    ...
    print(f"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")
CÃ¢u 2: ThÃªm táº§ng tÃ­ch cháº­p thá»© ba
YÃªu cáº§u: ThÃªm conv3 vÃ o mÃ´ hÃ¬nh.
class MNIST_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3)
        self.conv2 = nn.Conv2d(16, 32, 3)
        self.conv3 = nn.Conv2d(32, 64, 3)  # thÃªm conv3
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 1 * 1, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))  # thÃªm conv3
        x = x.view(-1, 64 * 1 * 1)
        x = self.fc1(x)
        return x
Hiá»‡u quáº£: Táº§ng conv3 giÃºp há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng phá»©c táº¡p hÆ¡n, cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c.

CÃ¢u 3: Thay Ä‘á»•i learning rate
YÃªu cáº§u: Thá»­ learning rate 0.001 vÃ  0.1.

Káº¿t quáº£: LR tháº¥p há»c á»•n Ä‘á»‹nh nhÆ°ng cháº­m, LR cao há»c nhanh nhÆ°ng dá»… dao Ä‘á»™ng.
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # LR tháº¥p
# hoáº·c
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)  # LR cao
CÃ¢u 4: Trá»±c quan hÃ³a feature map táº§ng conv2
YÃªu cáº§u: Váº½ thÃªm feature map tá»« conv2.
def visualize_feature_map():
    model.eval()
    images, _ = next(iter(test_loader))
    img = images[0].unsqueeze(0).to(device)

    conv1_output = torch.relu(model.conv1(img))
    conv2_output = torch.relu(model.conv2(model.pool(conv1_output)))

    plt.figure(figsize=(20, 4))

    plt.subplot(1, 5, 1)
    plt.title("áº¢nh gá»‘c")
    plt.imshow(img.cpu().squeeze(), cmap='gray')
    plt.axis('off')

    plt.subplot(1, 5, 2)
    plt.title("Feature Map conv1 - 1")
    plt.imshow(conv1_output[0, 0].cpu().detach().numpy(), cmap='gray')
    plt.axis('off')

    plt.subplot(1, 5, 3)
    plt.title("Feature Map conv1 - 2")
    plt.imshow(conv1_output[0, 1].cpu().detach().numpy(), cmap='gray')
    plt.axis('off')

    plt.subplot(1, 5, 4)
    plt.title("Feature Map conv2 - 1")
    plt.imshow(conv2_output[0, 0].cpu().detach().numpy(), cmap='gray')
    plt.axis('off')

    plt.subplot(1, 5, 5)
    plt.title("Feature Map conv2 - 2")
    plt.imshow(conv2_output[0, 1].cpu().detach().numpy(), cmap='gray')
    plt.axis('off')

    plt.show()
Ã nghÄ©a: Feature map conv2 chi tiáº¿t hÆ¡n, trá»«u tÆ°á»£ng hÆ¡n so vá»›i conv1.

Káº¿t luáº­n chung
Sá»‘ lÆ°á»£ng epoch, kiáº¿n trÃºc máº¡ng, learning rate Ä‘á»u áº£nh hÆ°á»Ÿng rÃµ rá»‡t Ä‘áº¿n káº¿t quáº£.

TÄƒng Ä‘á»™ sÃ¢u máº¡ng giÃºp há»c Ä‘áº·c trÆ°ng phá»©c táº¡p hÆ¡n.

Learning rate cáº§n Ä‘iá»u chá»‰nh phÃ¹ há»£p Ä‘á»ƒ cÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh.

Trá»±c quan feature map giÃºp hiá»ƒu cÃ¡ch máº¡ng há»c vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng.
